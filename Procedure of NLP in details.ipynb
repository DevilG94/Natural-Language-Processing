{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b175d9d2",
   "metadata": {},
   "source": [
    "## Besic Techniques:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f1863",
   "metadata": {},
   "source": [
    "#### (1) Lowering Case ----->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "496fb37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: What is the STEP by step guide to invest In share market in india?\n",
      "------------------------------------------------------------------------------\n",
      "Lowered Sentence: what is the step by step guide to invest in share market in india?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What is the STEP by step guide to invest In share market in india?\"\n",
    "\n",
    "sentence_lower = str(sentence).lower()\n",
    "\n",
    "print(\"Original Sentence:\",sentence)\n",
    "\n",
    "print(\"--\"*39)\n",
    "\n",
    "print(\"Lowered Sentence:\",sentence_lower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8c88d",
   "metadata": {},
   "source": [
    "#### (2) Removing Puntuations ---->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0106d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d8383ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d322f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6620bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Questions Similarity ^ . We are actually happy !! Because we wanted this project * *\"\n",
    "\n",
    "without_punc = [word for word in sentence.split(\" \") if word not in list(punc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f0c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Questions Similarity ^ . We are actually happy !! Because we wanted this project * *\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acba2463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence without Punctuations : ----> Hello Everyone, this is team Data Dynamos We are got an project of Quora Questions Similarity We are actually happy !! Because we wanted this project\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence without Punctuations : ---->\",\" \".join(without_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9f55f",
   "metadata": {},
   "source": [
    "#### (3) Removing Special Characters & Nos---->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71987200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69143262",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\"\n",
    "\n",
    "sentence_clean = re.sub(\"[^a-zA-Z]\",\" \",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0ad5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b26386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Sentence :---> Find the remainder when  math          math  is divided by       \n"
     ]
    }
   ],
   "source": [
    "print(\"Clean Sentence :--->\",sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116828c",
   "metadata": {},
   "source": [
    "#### (4) Removal of HTML Tags --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f49dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''<h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>'''\n",
    "\n",
    "clean_sentence = re.sub(\"<.*?>\",\"\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfde0a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> <h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0878fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Sentence :---> Hello Guys How Are You\n"
     ]
    }
   ],
   "source": [
    "print(\"Clean Sentence :--->\",clean_sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fdcdfc",
   "metadata": {},
   "source": [
    "#### (5) Removing URL's --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "132ab4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25400\\1586168022.py:3: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  clean_sentence =re.sub(\"(http|https|www)\\S+\",\"\",sentence)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis (https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis) link and I found very interesting sentiment analysis projects\"\n",
    "\n",
    "clean_sentence =re.sub(\"(http|https|www)\\S+\",\"\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81eebf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis (https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Sentiment%20Analysis) link and I found very interesting sentiment analysis projects\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47f0db2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Sentence: I visited  ( link and I found very interesting sentiment analysis projects\n"
     ]
    }
   ],
   "source": [
    "print(\"Clean Sentence:\",clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6abd6e",
   "metadata": {},
   "source": [
    "#### (6) Removing extra spaces --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12f8b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hi Team Data Dynamos, How is your project going on ?\"\n",
    "\n",
    "clean_sentence = re.sub(\" +\",\" \",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a928be4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> Hi Team Data Dynamos, How is your project going on ?\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c084b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Sentence :---> Hi Team Data Dynamos, How is your project going on ?\n"
     ]
    }
   ],
   "source": [
    "print(\"Clean Sentence :--->\",clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ec93b",
   "metadata": {},
   "source": [
    "#### (7) Expanding Contraction --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c87dbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a8495b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We've reached final step of our data science internship. We'll meet u in project presentation.\"\n",
    "\n",
    "clear_sentence = contractions.fix(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43514a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :--> We've reached final step of our data science internship. We'll meet u in project presentation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :-->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "211f3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear Sentence :---> We have reached final step of our data science internship. We will meet you in project presentation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Clear Sentence :--->\",clear_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62ee78",
   "metadata": {},
   "source": [
    "#### (8) Text Correction ---->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf5e46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "776a688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0b0fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob = TextBlob(sentence)\n",
    "\n",
    "correct_sentence = textblob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0e2e2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :-->  We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--> \",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b55c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Sentence :---> He have reached final step of our data science Training. He'll meet you in project presentation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct Sentence :--->\",correct_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd704f",
   "metadata": {},
   "source": [
    "# Advanced Techniques :-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c1a589",
   "metadata": {},
   "source": [
    "### (1) Apply Tokenization -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485a1b7",
   "metadata": {},
   "source": [
    "#### (a) Sentence Tokenization:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10b39a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nk\n",
    "#import tokenize as tnk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab924b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bedc2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f56f7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b89970e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c8fe1f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens :---> ['Our Team name is Team Data Dynamos and we have selected Quora question similarity project.', 'We have started working on this project from 13th of May only.', 'Working with team gives little extra space to apply new things.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence Tokens :--->\",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538fb24",
   "metadata": {},
   "source": [
    "#### (b) Word Tokenization:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9134f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77229e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a9c1fece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc0da0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens :---> ['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question', 'similarity', 'project.?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Word Tokens :--->\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "16946c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "02fab1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cc23afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "630b5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> Our Team name is Team Data Dynamos and we have selected Quora question similarity project.?\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5c43905b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens :---> ['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question', 'similarity', 'project', '.', '?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Word Tokens :--->\",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9dc3ba",
   "metadata": {},
   "source": [
    "#### (c) Sub-Word(n-gram character) Tokenization:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc837568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4336dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "40310c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_tokens =list(ngrams((sentence.split(\" \")),n = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8d774cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence :---> Our Team name is Team Data Dynamos and we have selected Quora question similarity project. We have started working on this project from 13th of May only. Working with team gives little extra space to apply new things.\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79a8f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram Tokens :---> [('Our', 'Team', 'name'), ('Team', 'name', 'is'), ('name', 'is', 'Team'), ('is', 'Team', 'Data'), ('Team', 'Data', 'Dynamos'), ('Data', 'Dynamos', 'and'), ('Dynamos', 'and', 'we'), ('and', 'we', 'have'), ('we', 'have', 'selected'), ('have', 'selected', 'Quora'), ('selected', 'Quora', 'question'), ('Quora', 'question', 'similarity'), ('question', 'similarity', 'project.'), ('similarity', 'project.', 'We'), ('project.', 'We', 'have'), ('We', 'have', 'started'), ('have', 'started', 'working'), ('started', 'working', 'on'), ('working', 'on', 'this'), ('on', 'this', 'project'), ('this', 'project', 'from'), ('project', 'from', '13th'), ('from', '13th', 'of'), ('13th', 'of', 'May'), ('of', 'May', 'only.'), ('May', 'only.', 'Working'), ('only.', 'Working', 'with'), ('Working', 'with', 'team'), ('with', 'team', 'gives'), ('team', 'gives', 'little'), ('gives', 'little', 'extra'), ('little', 'extra', 'space'), ('extra', 'space', 'to'), ('space', 'to', 'apply'), ('to', 'apply', 'new'), ('apply', 'new', 'things.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"N-gram Tokens :--->\",n_gram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455de6f4",
   "metadata": {},
   "source": [
    "## (2) Remove Stop Words:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e894f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3cadb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bd4105d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stop Words in English= 179\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Stop Words in English=\",len(stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d84d298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Our Team name is Team Data Dynamos and we have selected Quora question similarity project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f407027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_non_stopword =[word for word in sentence.split(\" \") if not word in stopwords_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9cd93120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with StopWOrds :---> Our Team name is Team Data Dynamos and we have selected Quora question similarity project\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence with StopWOrds :--->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "21dbf88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence without StopWOrds :---> Our Team name Team Data Dynamos selected Quora question similarity project\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence without StopWOrds :--->\",\" \".join(sentence_non_stopword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b1eca",
   "metadata": {},
   "source": [
    "### (3) Apply Stemming:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa94023-b1dc-4ba4-99ef-12ad5850f694",
   "metadata": {},
   "source": [
    "(a) Porter Stemmer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6e07f443-e5bf-479a-9d9f-47ae86303ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7c8d82a8-e732-45e8-beeb-fa00c201735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "666ae028-f76d-4753-bd5d-5af045393d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f9f7f77c-1dc8-47cc-bcf9-cb44ca82ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stem =[porter.stem(word) for word in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d3c26c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence:\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "90937c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect drive driven drive abl abl enabl enabl enabl\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence after Porter Stemming:\",\" \".join(porter_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f116f5a-e6bd-4ac4-bf7b-a12fff0a395d",
   "metadata": {},
   "source": [
    "(b) Snowball Stemmer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8af83285-cfdd-4a86-819f-02471c0a629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74b1b498-e418-4bde-9d4f-ce54222d9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language =\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "64be616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f97d278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stem = [snowball.stem(word) for word in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "55fb1325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence:\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bcb23e29-a697-4510-a7ed-3c14f6002fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect drive driven drive abl abl enabl enabl enabl\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence after Porter Stemming:\",\" \".join(snowball_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f96ed6-c5e7-402f-ab5c-039b377a3705",
   "metadata": {},
   "source": [
    "(c) Lancaster Stemmer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8fe94225-b3f3-4dc9-88cc-30f3646d9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e6e51d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "82228fcd-3796-4850-a367-6b095148cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1715f871-fcdd-42ff-b8b1-ff0353986453",
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster_stem = [lancaster.stem(word) for word in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "72eb6471-05b8-4519-80c1-fcd09de0b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence:\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "51fd7e9a-f968-45ce-ba63-95bdcbb7963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after Porter Stemming: connect connect connect connect connect connect connect driv driv driv abl abl en en en\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence after Porter Stemming:\",\" \".join(lancaster_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16a28e-887e-4206-a7d1-35383e476ab6",
   "metadata": {},
   "source": [
    "(d) Regexp Stemmer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "23124ae4-2314-491e-884a-d3bfbabcd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ba7354f2-e5f5-4824-8d02-93ed8a62c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex =RegexpStemmer(regexp = \"ing$|s$|e$\",min = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e9446126-7b31-4385-b14b-6a4831df89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cf0a7219-0061-4997-b69a-f0cb76d392e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_stem =[regex.stem(word) for word in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ae660632-d5ed-4d87-b089-59e51132a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence:\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cc71513c-45b3-434b-bc7c-2747fc811184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after Porter Stemming: Connect Connection Connection Connect Connected Connect Connecting Driv Driven Drive Abl Able Enabl Enable Enabl\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence after Porter Stemming:\",\" \".join(regex_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e94a4-1aa3-4878-b1e0-58853bb22e15",
   "metadata": {},
   "source": [
    "(4) Apply Lemmatization:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9054daa-b4ce-400e-a826-243a2f27a8f9",
   "metadata": {},
   "source": [
    "(i) Wordnet Lematization;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "86f0e136-6771-4bb0-aa88-5c323220e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "49670cba-bcdb-4e0d-a25a-183c5dae9d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e4f2c31f-bddb-4541-8814-495255f635a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The bats are hanging on their feet in upright positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5b99c7db-5b17-4166-a53a-54c7a8c941d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lemma = [lemma.lemmatize(word,'v') for word in sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2a692fe5-e771-4a0e-b32b-260728fd17ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence ---> The bats are hanging on their feet in upright positions\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence --->\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0f66a328-2cc1-4f7d-944c-ff0b89ca54c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after Lemmatization --> The bat be hang on their feet in upright position\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence after Lemmatization -->\",\" \".join(sentence_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab4aa7-3529-4fc0-a4d3-0d90d7fb16bb",
   "metadata": {},
   "source": [
    "(ii) TextBlob Lemmatizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c117ff25-4b49-4e15-bc3d-b852a85ed5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob,Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a4ffbbab-d78a-4ad3-9369-368373da8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The bats are hanging on their feet in upright positions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e21c32f9-08bd-4cca-ab17-263b1e56d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = TextBlob(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "77db6960-0a99-4be9-a90f-7757e6f03f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "texblob_lemma = [w.lemmatize() for w in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b7d1205e-e3e2-40d9-890b-270bf23cecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The bats are hanging on their feet in upright positions\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Sentence:\",sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b38a4a38-7abe-42c6-90d1-0d88464b3cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after Lemmatization ---> The bat are hanging on their foot in upright position\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence after Lemmatization --->\",\" \".join(texblob_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f00993-64ef-41f3-a5e3-eaddc5684f72",
   "metadata": {},
   "source": [
    "More Advanced Techniques:-"
   ]
  },
  {
   "cell_type": "raw",
   "id": "646a2d87-fad8-4554-afdb-c52a13fc3346",
   "metadata": {},
   "source": [
    "1) POS Tagging (Part of Speech);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d764065-9dc8-4fda-bca9-d1f990394532",
   "metadata": {},
   "source": [
    "(a) POS Tagging using NLTK;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d93cb2e9-701b-4caa-9849-b5fc55d3ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9f5c2597-7d90-4346-a43d-6f4b967a5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = word_tokenize(\"What is the step by step guide to invest in share market in india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8e25fb4d-e942-485e-8c17-123edd531aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS Tag: WP\n",
      "Word: is || POS Tag: VBZ\n",
      "Word: the || POS Tag: DT\n",
      "Word: step || POS Tag: NN\n",
      "Word: by || POS Tag: IN\n",
      "Word: step || POS Tag: NN\n",
      "Word: guide || POS Tag: RB\n",
      "Word: to || POS Tag: TO\n",
      "Word: invest || POS Tag: VB\n",
      "Word: in || POS Tag: IN\n",
      "Word: share || POS Tag: NN\n",
      "Word: market || POS Tag: NN\n",
      "Word: in || POS Tag: IN\n",
      "Word: india || POS Tag: NN\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(doc)):\n",
    "    print(\"Word:\",pos_tag(doc)[i][0],\"||\",\"POS Tag:\",pos_tag(doc)[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aaa04e-4025-4be9-97d0-771ce30f1369",
   "metadata": {},
   "source": [
    "(b) POS Tagging using Spacy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d307c680-aa04-4f15-a9ff-7c2893f56720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2614348-8d31-44ab-b982-0d7920960f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "doc=nlp(\"What is the step by step guide to invest in share market in india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f905f95f-ced7-4b72-a4a4-e8aa91db098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: What || POS: PRON || POS Tag: WP ||\n",
      "Word: is || POS: AUX || POS Tag: VBZ ||\n",
      "Word: the || POS: DET || POS Tag: DT ||\n",
      "Word: step || POS: NOUN || POS Tag: NN ||\n",
      "Word: by || POS: ADP || POS Tag: IN ||\n",
      "Word: step || POS: NOUN || POS Tag: NN ||\n",
      "Word: guide || POS: NOUN || POS Tag: NN ||\n",
      "Word: to || POS: PART || POS Tag: TO ||\n",
      "Word: invest || POS: VERB || POS Tag: VB ||\n",
      "Word: in || POS: ADP || POS Tag: IN ||\n",
      "Word: share || POS: NOUN || POS Tag: NN ||\n",
      "Word: market || POS: NOUN || POS Tag: NN ||\n",
      "Word: in || POS: ADP || POS Tag: IN ||\n",
      "Word: india || POS: PROPN || POS Tag: NNP ||\n"
     ]
    }
   ],
   "source": [
    "for word in doc:\n",
    "   print(\"Word:\",word.text,\"||\",\"POS:\",word.pos_,\"||\",\"POS Tag:\",word.tag_,\"||\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc0701a4-66d6-4879-bcb8-96b325c61f01",
   "metadata": {},
   "source": [
    "Spacy is more powerful than NLTK. Spacy is faster and Grammatically accurate......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0a114-f7c2-437f-baa3-6a454eb68b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1489bbf-a19f-4fbd-993a-5db6affd527a",
   "metadata": {},
   "source": [
    "(2) NER Tagging:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b39cac-4f8c-4e3c-b7f1-a1aa28e64844",
   "metadata": {},
   "source": [
    "a) NER using NLTK;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e08adf2e-3db7-4fe1-baaf-299726d38832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9065c487-e098-4e6b-b079-640049fa573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "4d79e400-36fc-434f-8042-ca80c5f12664",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"TATA and Mahindra are the top companies in India. But the 'Gautam Adani' and 'Mukesh Ambani' 'reachest person.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ce174bd1-b9e1-4858-b394-5bae952efb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in sentence.split(\" \") if word not in stopwords_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "668a4dcb-36ab-40b2-bbf7-8e50d241148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1e8fb92b-4404-4129-bcd9-8d686c82ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = nltk.ne_chunk(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "65fb0b7f-28c5-491f-bbb4-e8cb1ff0c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ORGANIZATION TATA/NNP Mahindra/NNP)\n",
      "('top', 'JJ')\n",
      "('companies', 'NNS')\n",
      "('India.', 'NNP')\n",
      "('But', 'CC')\n",
      "(\"'Gautam\", 'NNP')\n",
      "(\"Adani'\", 'NNP')\n",
      "(\"'Mukesh\", 'POS')\n",
      "(\"Ambani'\", 'NNP')\n",
      "(\"'reachest\", 'JJS')\n",
      "('person.', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for entity in entities:\n",
    "    print (entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553651fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) NER Using Spacy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5bb2371-082e-45d0-9a17-62b784d9fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb9097d-d2be-4591-9c89-55116cc52b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15dc818-ed06-4a0c-bf7c-7a0a0a2aac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =\"TATA and Mahindra are the top companies in India. But the 'Gautam Adani' and 'Mukesh Ambani'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45aa7d8-2e59-4c5f-aad9-e58a603033fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f39c1687-107b-4236-8584-b37760d93558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TATA ORG\n",
      "Mahindra ORG\n",
      "India GPE\n",
      "Gautam Adani' PERSON\n",
      "Mukesh Ambani' PERSON\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text,entity.label_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1122fe7-2c5c-48c2-9ac0-0341b789ed82",
   "metadata": {},
   "source": [
    "Spacy is a faster and more efficient library for NER. It provides a pre-trained NER model that is highly accurate than NLTK and can recognize a wide range of named entities. Additionally, SpaCy has more advanced features such as named entity linking and coreference resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596dc099-3075-45e6-8e4e-3562720ecde9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
